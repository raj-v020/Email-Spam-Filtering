{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1vNqjmEl4eSn9tL1blos5e15KhHzt6HUb","authorship_tag":"ABX9TyO5d6Xl+bRLfsG6lShZonEr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":59,"metadata":{"id":"Wy1-jrEbl9YS","executionInfo":{"status":"ok","timestamp":1728483612818,"user_tz":-330,"elapsed":848,"user":{"displayName":"Raj Vishwakarma","userId":"08775079886412699309"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import nltk\n","from math import log\n","from nltk.corpus import stopwords\n","from nltk import PorterStemmer as Stemmer\n","import re"]},{"cell_type":"code","source":["nltk.download('stopwords')"],"metadata":{"id":"EDKhqCWHoPpU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import zipfile\n","\n","zip_ref = zipfile.ZipFile('/content/drive/MyDrive/data.zip', 'r')\n","zip_ref.extractall('/content/dataset')\n","zip_ref.close()"],"metadata":{"id":"q2QKP9kTJIYm","executionInfo":{"status":"ok","timestamp":1728483613464,"user_tz":-330,"elapsed":651,"user":{"displayName":"Raj Vishwakarma","userId":"08775079886412699309"}}},"execution_count":61,"outputs":[]},{"cell_type":"code","source":["feature_index = 0\n","target_index = 1"],"metadata":{"id":"WIcMvrvO5Iht","executionInfo":{"status":"ok","timestamp":1728483613465,"user_tz":-330,"elapsed":12,"user":{"displayName":"Raj Vishwakarma","userId":"08775079886412699309"}}},"execution_count":62,"outputs":[]},{"cell_type":"code","source":["def count_words(message):\n","  counts = {}\n","  for word in message:\n","    if word in counts:\n","      counts[word] += 1\n","    else:\n","      counts[word] = 1\n","  return counts"],"metadata":{"id":"-hmBZTN0iq_V","executionInfo":{"status":"ok","timestamp":1728483613465,"user_tz":-330,"elapsed":11,"user":{"displayName":"Raj Vishwakarma","userId":"08775079886412699309"}}},"execution_count":63,"outputs":[]},{"cell_type":"code","source":["def process_kaggle(split):\n","  data = pd.read_csv('/content/dataset/emails.csv').to_numpy()\n","  np.random.shuffle(data)\n","  data_split = int(len(data)*split)\n","  train_data, test_data = (data[:data_split], data[data_split:])\n","  total_prob_ham = len([d for d in data if d[target_index] == 0])/len(data)\n","  total_prob_spam = 1 - total_prob_ham\n","  return train_data, test_data, total_prob_ham, total_prob_spam"],"metadata":{"id":"AhInW-Mb4Aoh","executionInfo":{"status":"ok","timestamp":1728483613465,"user_tz":-330,"elapsed":10,"user":{"displayName":"Raj Vishwakarma","userId":"08775079886412699309"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","source":["def process_enron(split):\n","  data = pd.read_csv('/content/dataset/enron_spam_data.csv')\n","  data = data[['Subject', 'Message', 'Spam/Ham']] #Select only these columns\n","  data = data[data['Message'].notna() & data['Subject'].notna() & data['Spam/Ham'].notna()] #filter out NaN data\n","  data['Mail'] = data['Subject'] + ' ' + data['Message'] #Combine Subject with Mail Message\n","  data['Spam/Ham'] = data['Spam/Ham'].replace(['ham', 'spam'], [0, 1]) #Convert text labels to numbers\n","  data = data[['Mail', 'Spam/Ham']] #Select only these columns\n","  data.columns = ['Message', 'Spam/Ham'] #Rename columns\n","  data = data.to_numpy()\n","  np.random.shuffle(data)\n","  data_split = int(len(data)*split)\n","  train_data, test_data = (data[:data_split], data[data_split:])\n","  total_prob_ham = len([d for d in data if d[target_index] == 0])/len(data)\n","  total_prob_spam = 1 - total_prob_ham\n","  return train_data, test_data, total_prob_ham, total_prob_spam"],"metadata":{"id":"ob5sTyLW5Ukg","executionInfo":{"status":"ok","timestamp":1728483613466,"user_tz":-330,"elapsed":11,"user":{"displayName":"Raj Vishwakarma","userId":"08775079886412699309"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":["def process_mix(split):\n","  data = pd.read_csv('/content/dataset/data.csv').to_numpy()\n","  np.random.shuffle(data)\n","  data_split = int(len(data)*split)\n","  train_data, test_data = (data[:data_split], data[data_split:])\n","  total_prob_ham = len([d for d in data if d[target_index] == 0])/len(data)\n","  total_prob_spam = 1 - total_prob_ham\n","  return train_data, test_data, total_prob_ham, total_prob_spam"],"metadata":{"id":"J_luBPy1q0Xt","executionInfo":{"status":"ok","timestamp":1728483613466,"user_tz":-330,"elapsed":10,"user":{"displayName":"Raj Vishwakarma","userId":"08775079886412699309"}}},"execution_count":66,"outputs":[]},{"cell_type":"code","source":["class NB_Classifier(object):\n","\n","    def __init__(self, train_data, test_data):\n","        self.hams = [h[feature_index] for h in train_data if h[target_index] == 0]\n","        self.spams = [s[feature_index] for s in train_data if s[target_index] == 1]\n","        self.stop_words = stopwords.words('english')\n","        self.words = {}\n","        self.get_words(self.hams + self.spams)\n","\n","    def get_words(self, docs):\n","        word_index = 0\n","        for doc in docs:\n","            tokens = self.tokenize(doc)\n","            for token in tokens:\n","                if token not in self.words:\n","                    self.words[token] = word_index\n","                    word_index += 1\n","\n","    def tfidf(self, docs):\n","        num_docs = len(docs)\n","        num_words = len(self.words)\n","        F = np.zeros((num_docs, num_words))\n","\n","        for i, doc in enumerate(docs):\n","            tokens = self.tokenize(doc)\n","            token_counts = count_words(tokens)\n","            for token, count in token_counts.items():\n","                if token in self.words:\n","                    j = self.words[token]\n","                    if(count != 0):\n","                      F[i, j] = 1\n","        return F\n","\n","    def train(self):\n","      ham_size = len(self.hams)\n","      spam_size = len(self.spams)\n","\n","      self.ham_probs = (self.tfidf(self.hams).sum(axis = 0) + 1) / (ham_size + 2)\n","      self.spam_probs = (self.tfidf(self.spams).sum(axis = 0) + 1) / (spam_size + 2)\n","\n","\n","    def test(self):\n","      confusion_mat = np.zeros((2, 2), dtype=int)\n","      for mail in test_data:\n","        prediction = self.classify_mail(mail[feature_index])\n","        target = mail[target_index]\n","        if prediction == target:\n","          confusion_mat[prediction][prediction] += 1\n","        else:\n","          confusion_mat[prediction][target] += 1\n","\n","      print(confusion_mat)\n","\n","    def classify_mail(self, mail):\n","\n","      mail = self.tokenize(mail)\n","      prob_ham = log(total_prob_ham)\n","      prob_spam = log(total_prob_spam)\n","\n","      for word in mail:\n","        if word in self.words:\n","          index = self.words[word]\n","          prob_ham = prob_ham + log(self.ham_probs[index])\n","          prob_spam = prob_spam + log(self.spam_probs[index])\n","\n","      if prob_spam >= prob_ham:\n","        return 1\n","      else:\n","        return 0\n","\n","    def tokenize(self, doc):\n","        stemmer = Stemmer()\n","        tokens = re.findall(r'\\b\\w+\\b', doc.lower())\n","        if self.stop_words:\n","            tokens = [stemmer.stem(t) for t in tokens if t not in self.stop_words]\n","        return np.unique(tokens)\n","\n"],"metadata":{"id":"sUkipWwxmicD","executionInfo":{"status":"ok","timestamp":1728483613466,"user_tz":-330,"elapsed":10,"user":{"displayName":"Raj Vishwakarma","userId":"08775079886412699309"}}},"execution_count":67,"outputs":[]},{"cell_type":"code","source":["train_data, test_data, total_prob_ham, total_prob_spam = process_enron(0.8)"],"metadata":{"id":"ULPYcZqCmmAl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = NB_Classifier(train_data, test_data)"],"metadata":{"id":"-VpRzYkwfkUj","executionInfo":{"status":"ok","timestamp":1728483751026,"user_tz":-330,"elapsed":135201,"user":{"displayName":"Raj Vishwakarma","userId":"08775079886412699309"}}},"execution_count":69,"outputs":[]},{"cell_type":"code","source":["model.train()"],"metadata":{"id":"JAY3SntHag3w","executionInfo":{"status":"ok","timestamp":1728483888028,"user_tz":-330,"elapsed":137034,"user":{"displayName":"Raj Vishwakarma","userId":"08775079886412699309"}}},"execution_count":70,"outputs":[]},{"cell_type":"code","source":["model.test()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Hj5KmS6iuBO","executionInfo":{"status":"ok","timestamp":1728483921117,"user_tz":-330,"elapsed":33120,"user":{"displayName":"Raj Vishwakarma","userId":"08775079886412699309"}},"outputId":"fa26fd26-3942-4a71-c214-ebe117fa7eb2"},"execution_count":71,"outputs":[{"output_type":"stream","name":"stdout","text":["[[3283   47]\n"," [  30 3262]]\n"]}]},{"cell_type":"code","source":["!git add spam-filter.ipyn"],"metadata":{"id":"8bf2qnP4XpMp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gUmXf8D2BdPv","executionInfo":{"status":"ok","timestamp":1728483921118,"user_tz":-330,"elapsed":12,"user":{"displayName":"Raj Vishwakarma","userId":"08775079886412699309"}}},"execution_count":71,"outputs":[]}]}