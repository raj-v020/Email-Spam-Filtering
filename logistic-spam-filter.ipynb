{"cells":[{"cell_type":"code","execution_count":167,"metadata":{"executionInfo":{"elapsed":621,"status":"ok","timestamp":1728567567270,"user":{"displayName":"Raj Vishwakarma","userId":"08775079886412699309"},"user_tz":-330},"id":"fpHo8o8Ys4_K"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import nltk\n","from math import log, sqrt\n","from nltk.corpus import stopwords\n","from nltk import PorterStemmer as Stemmer\n","import re\n","import pickle\n","import os"]},{"cell_type":"code","execution_count":168,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1728567567864,"user":{"displayName":"Raj Vishwakarma","userId":"08775079886412699309"},"user_tz":-330},"id":"JmAc5mZ69qyJ","outputId":"a1f979ba-0c8e-4eee-e700-6a33c33921a9"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":168}],"source":["nltk.download('stopwords')"]},{"cell_type":"code","execution_count":169,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1728567567864,"user":{"displayName":"Raj Vishwakarma","userId":"08775079886412699309"},"user_tz":-330},"id":"pEJC2yTgu0Dk"},"outputs":[],"source":["# import zipfile\n","\n","# zip_ref = zipfile.ZipFile('/content/drive/MyDrive/data.zip', 'r')\n","# zip_ref.extractall('/content/dataset')\n","# zip_ref.close()"]},{"cell_type":"code","execution_count":170,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1728567567864,"user":{"displayName":"Raj Vishwakarma","userId":"08775079886412699309"},"user_tz":-330},"id":"KkqR0EYq922S"},"outputs":[],"source":["def process_kaggle(split):\n","  data = pd.read_csv('/content/dataset/emails.csv').to_numpy()\n","  np.random.shuffle(data)\n","  data_split = int(len(data)*split)\n","  train_data, test_data = (data[:data_split], data[data_split:])\n","  return train_data, test_data"]},{"cell_type":"code","execution_count":171,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1728567567864,"user":{"displayName":"Raj Vishwakarma","userId":"08775079886412699309"},"user_tz":-330},"id":"min6zH7c5MUk"},"outputs":[],"source":["def process_enron(split):\n","  data = pd.read_csv('/content/dataset/enron_spam_data.csv')\n","  data = data[['Subject', 'Message', 'Spam/Ham']] #Select only these columns\n","  data = data[data['Message'].notna() & data['Subject'].notna() & data['Spam/Ham'].notna()] #filter out NaN data\n","  data['Mail'] = data['Subject'] + ' ' + data['Message'] #Combine Subject with Mail Message\n","  data['Spam/Ham'] = data['Spam/Ham'].replace(['ham', 'spam'], [0, 1]) #Convert text labels to numbers\n","  data = data[['Mail', 'Spam/Ham']] #Select only these columns\n","  data.columns = ['Message', 'Spam/Ham'] #Rename columns\n","  data = data.to_numpy()\n","  np.random.shuffle(data)\n","  data_split = int(len(data)*split)\n","  train_data, test_data = (data[:data_split], data[data_split:])\n","  return train_data, test_data"]},{"cell_type":"code","execution_count":172,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1728567567865,"user":{"displayName":"Raj Vishwakarma","userId":"08775079886412699309"},"user_tz":-330},"id":"KeH0PmdZqKqA"},"outputs":[],"source":["class LogisticRegression():\n","  def __init__(self):\n","    self.stop_words = stopwords.words('english')\n","    self.words = {}\n","\n","  def tokenize(self, doc):\n","      stemmer = Stemmer()\n","      tokens = re.findall(r'\\b\\w+\\b', doc.lower())\n","      if self.stop_words:\n","          tokens = [stemmer.stem(t) for t in tokens if t not in self.stop_words]\n","      return np.unique(tokens)\n","\n","  def get_words(self, docs):\n","      word_index = 0\n","      for doc in docs:\n","          tokens = self.tokenize(doc)\n","          for token in tokens:\n","              if token not in self.words:\n","                  self.words[token] = word_index\n","                  word_index += 1\n","\n","  def vectorize(self, x):\n","    vector = np.zeros(len(self.words.keys()))\n","    for token in self.tokenize(x):\n","      if token in self.words:\n","        vector[self.words[token]] = 1\n","    return vector\n","\n","  def train(self, train_data, epochs, rate, reg_param, batch_size=None):\n","    self.get_words(train_data[:, 0])\n","    self.train_mails = [self.vectorize(x[0]) for x in train_data]\n","    self.train_targets = train_data[:, 1]\n","    self.train_size = len(self.train_targets)\n","    self.reg_param = reg_param\n","    self.weights = np.zeros(len(self.words.keys()), dtype=float)\n","    self.bias = 0.0\n","\n","    if batch_size:\n","      self.mail_batches = [self.train_mails[k:k+batch_size]\n","                  for k in range(0, self.train_size, batch_size)]\n","      self.target_batches = [self.train_targets[k:k+batch_size]\n","                  for k in range(0, self.train_size, batch_size)]\n","\n","    for e in range(epochs):\n","      if batch_size:\n","        self.Stochastic_Gradient_Ascent(rate)\n","      else:\n","        self.Gradient_Ascent(rate)\n","\n","      if e % int(epochs*0.1) == 0:\n","        print(\"epoch: \", e, \"\\tlikelihood: \", self.total_likelihood(), \"\\tL2 likelihood: \", self.total_likelihood_l2())\n","\n","    print(self.weights)\n","    print(self.bias)\n","\n","  def test(self, test_data):\n","    test_mails = test_data[:, 0]\n","    test_targets = test_data[:, 1]\n","    test_size = len(test_targets)\n","\n","    confusion_mat = np.zeros((2, 2), dtype=int)\n","    for mail, target in zip(test_mails, test_targets):\n","      prediction = self.predict(mail)\n","      prediction = 0 if prediction < 0.5 else 1\n","      confusion_mat[prediction][target] += 1\n","\n","    print(\"Confusion Matrix:\")\n","    print(confusion_mat)\n","\n","    # Calculate statistics\n","    true_positive = confusion_mat[1][1]\n","    true_negative = confusion_mat[0][0]\n","    false_positive = confusion_mat[1][0]\n","    false_negative = confusion_mat[0][1]\n","\n","    accuracy = (true_positive + true_negative) / np.sum(confusion_mat)\n","    precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n","    recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0\n","    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n","\n","    print(f\"Accuracy: {accuracy:.4f}\")\n","    print(f\"Precision: {precision:.4f}\")\n","    print(f\"Recall: {recall:.4f}\")\n","    print(f\"F1 Score: {f1_score:.4f}\")\n","\n","  def Gradient_Ascent(self, rate):\n","    dll_dw = np.zeros(len(self.weights), dtype=float)\n","    dll_db = 0.0\n","    for x, y in zip(self.train_mails, self.train_targets):\n","      z = np.dot(x, self.weights) + self.bias\n","      f = self.sigmoid(z)\n","      f_prime = f*(1 - f) # derivative of sigmoid function\n","      dll_db += (y/f)*f_prime - ((1 - y)/(1 - f))*f_prime\n","      dll_dw += dll_db*x\n","\n","    self.weights += rate*(dll_dw - self.reg_param*self.weights)/float(self.train_size)\n","    self.bias += rate*(dll_db - self.reg_param*self.bias)/float(self.train_size)\n","\n","  def Stochastic_Gradient_Ascent(self, rate):\n","    for mail_batch, target_batch in zip(self.mail_batches, self.target_batches):\n","      dll_dw = np.zeros(len(self.weights), dtype=float)\n","      dll_db = 0.0\n","      batch_len = len(mail_batch)\n","      for x, y in zip(mail_batch, target_batch):\n","        z = np.dot(x, self.weights) + self.bias\n","        f = self.sigmoid(z)\n","        f_prime = f*(1 - f) # derivative of sigmoid function\n","        dll_db += (y/f)*f_prime - ((1 - y)/(1 - f))*f_prime\n","        dll_dw += dll_db*x\n","\n","      self.weights += rate*(dll_dw - self.reg_param*self.weights)/float(batch_len)\n","      self.bias += rate*(dll_db - self.reg_param*self.bias)/float(batch_len)\n","\n","  def total_likelihood(self):\n","    likelihood = 0\n","    for mail, target in zip(self.train_mails, self.train_targets):\n","      likelihood += self.log_likelihood(mail, target)\n","\n","    return -likelihood/self.train_size\n","\n","  def total_likelihood_l2(self):\n","    return self.total_likelihood() + (self.reg_param/2)*(np.dot(self.weights, self.weights) + self.bias*self.bias)/self.train_size\n","\n","  def log_likelihood(self, x, y):\n","    z = np.dot(x, self.weights) + self.bias\n","    return y * log(self.sigmoid(z)) + (1 - y) * log(1 - self.sigmoid(z))\n","\n","  def predict(self, x):\n","    z = np.dot(self.vectorize(x), self.weights) + self.bias\n","    return self.sigmoid(z)\n","\n","  def sigmoid(self, z):\n","    return 1 / (1 + np.exp(-z))\n","\n","  def save(self, path):\n","    with open(path, 'wb') as f:\n","      pickle.dump(self, f)\n","\n","  @classmethod\n","  def load(cls, path):\n","      if os.path.exists(path):\n","        model = cls()\n","        with open(path, 'rb') as f:\n","          return pickle.load(f)\n","      else:\n","        print(\"Model parameters not saved: Cannot load the file, \", path)\n","        return\n"]},{"cell_type":"code","execution_count":173,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1728567567865,"user":{"displayName":"Raj Vishwakarma","userId":"08775079886412699309"},"user_tz":-330},"id":"9TACX1hZtCSE"},"outputs":[],"source":["# train_data, test_data = process_kaggle(0.8)\n","model_path = '/content/drive/MyDrive/Colab Notebooks/saved-models/logistic-kaggle-model.pkl'"]},{"cell_type":"code","execution_count":174,"metadata":{"id":"dMz0dSWg-HDu","executionInfo":{"status":"ok","timestamp":1728567567865,"user_tz":-330,"elapsed":5,"user":{"displayName":"Raj Vishwakarma","userId":"08775079886412699309"}}},"outputs":[],"source":["# model = LogisticRegression()\n","# model.train(train_data, epochs=50, rate=0.01, reg_param=0.1, batch_size = 30)\n","# model.test(test_data)"]},{"cell_type":"code","execution_count":175,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B0P8ew1-iRJz","executionInfo":{"status":"ok","timestamp":1728567567865,"user_tz":-330,"elapsed":5,"user":{"displayName":"Raj Vishwakarma","userId":"08775079886412699309"}},"outputId":"6b870248-01a9-4d0c-a74e-5be2f8845c2d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.31774501554554435"]},"metadata":{},"execution_count":175}],"source":["# model = LogisticRegression.load(model_path)"]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"18DAvQOepTXOGBTkm49S-gKhn-ZtMuYkg","authorship_tag":"ABX9TyMJaLlB4Fhz4gSaKaceiD/Y"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}